# 0x03. Optimization
***
## This is a README.md for the repository
### In GitHub [0x03-optimization]()
```
For Holberton School
Cohort 16.
```
## General

## Resources
* Hyperparameter (machine learning)
* Feature scaling
* Why, How and When to Scale your Features
* Normalizing your data
* Moving average
* An overview of gradient descent optimization algorithms
* A Gentle Introduction to Mini-Batch Gradient Descent and How to * Configure Batch Size
* Stochastic Gradient Descent with momentum
* Understanding RMSprop
* Adam
* Learning Rate Schedules
* deeplearning.ai videos (Note: I suggest watching these video at 1.5x - 2x speed):
* Normalizing Inputs
* Mini Batch Gradient Descent
* Understanding Mini-Batch Gradient Descent
* Exponentially Weighted Averages
* Understanding Exponentially Weighted Averages
* Bias Correction of Exponentially Weighted Averages
* Gradient Descent With Momentum
* RMSProp
* Adam Optimization Algorithm
* Learning Rate Decay
* Normalizing Activations in a Network
* Fitting Batch Norm Into Neural Networks
* Why Does Batch Norm Work?
* Batch Norm At Test Time
* The Problem of Local Optima

## References:

* numpy.random.permutation
* tf.nn.moments
* tf.train.MomentumOptimizer
* tf.train.RMSPropOptimizer
* tf.train.AdamOptimizer
* tf.nn.batch_normalization
* tf.train.inverse_time_decay

## Read or watch:

![Alt text]()

## Extra resources around relational:

## More Info

## Files included

| File                 | Details                                    |
|--------------------- | ------------------------------------------ |
| []() |	       |
| []() |	       |
| []() |	       |
| []() |	       |
| []() |	       |
| []() |	       |
| []() |	       |
| []() |	       |
| []() |	       |
| []() |	       |


## Author
***
*Holberton School Student*

Juan Sebastian Posada  - [Github](https://github.com/Juansepo13) - [Twiter](https://twitter.com/@JuanSeb35904130)
