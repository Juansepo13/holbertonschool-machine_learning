# 0x11. Attention
***
## This is a README.md for the repository
### In GitHub [0x11-attention]()
```
For Holberton School
Cohort 16.
```
## General

* What is the attention mechanism?
* How to apply attention to RNNs
* What is a transformer?
* How to create an encoder-decoder transformer model
* What is GPT?
* What is BERT?
* What is self-supervised learning?
* How to use BERT for specific NLP tasks
* What is SQuAD? GLUE?

## Resources
* [Attention for RNN Seq2Seq Models]()
* [Attention Model Intuition]()
* [Attention Model]()
* [How Transformers work in deep learning and NLP: an intuitive introduction]()
* [Transformers]()
* [Bert, GPT : The Illustrated GPT-2 - Visualizing Transformer Language Models]()
* [SQuAD]()
* [Glue]()
* [Self supervised learning]()
## Read or watch:

![Alt_text](https://s3.eu-west-3.amazonaws.com/hbtn.intranet/uploads/medias/2020/7/4704cf0750335400050c494f69844150e6319d1b.jpg?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIA4MYA5JM5DUTZGMZG%2F20230116%2Feu-west-3%2Fs3%2Faws4_request&X-Amz-Date=20230116T162832Z&X-Amz-Expires=86400&X-Amz-SignedHeaders=host&X-Amz-Signature=545330ee40964695c8efde193a880468b93a93914a5b0bf3bb8ded77f6dc34fe)

## Extra resources around relational:

## More Info

## Files included

| File                 | Details                                    |
|--------------------- | ------------------------------------------ |
| []() |	       |
| []() |	       |
| []() |	       |
| []() |	       |
| []() |	       |
| []() |	       |
| []() |	       |
| []() |	       |
| []() |	       |
| []() |	       |


## Author
***
*Holberton School Student*

Juan Sebastian Posada  - [Github](https://github.com/Juansepo13) - [Twiter](https://twitter.com/@JuanSeb35904130)