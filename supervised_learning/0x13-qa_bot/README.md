# 0x13. QA Bot
***
## This is a README.md for the repository
### In GitHub [0x13-qa_bot]()
```
For Holberton School
Cohort 16.
```
## General

* Allowed editors: vi, vim, emacs
* All your files will be interpreted/compiled on Ubuntu 16.04 LTS using python3 (version 3.6.12)
* Your files will be executed with numpy ```(version 1.18)``` and tensorflow ```(version 2.3)```
* All your files should end with a new line
* The first line of all your files should be exactly ```#!/usr/bin/env python3```
* All of your files must be executable
* A README.md file, at the root of the folder of the project, is mandatory
* Your code should follow the pycodestyle style (version 2.4)
* All your modules should have documentation ```(python3 -c 'print(__import__("my_module").__doc__)')```
* All your classes should have documentation ```(python3 -c 'print(__import__("my_module").MyClass.__doc__)')```
* All your functions (inside and outside a class) should have documentation ```(python3 -c 'print(__import__("my_module").my_function.__doc__)' and python3 -c 'print(__import__("my_module").MyClass.my_function.__doc__)')```

* ## Upgrade to Tensorflow 2.3
* ``` pip install --user tensorflow==2.3 ```

## Install Tensorflow Hub
* ``` pip install --user tensorflow-hub ```

## Install Transformers
* ``` pip install --user transformers ``` 

## Resources

### Read or watch:
* ## Improving Language Understanding by Generative Pre-Training (2018)
* BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (2018)
* SQuAD 2.0
* Know What You Donâ€™t Know: Unanswerable Questions for SQuAD (2018)
* GLUE Benchmark
* GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding (2019)
* Speech-transformer: A no-recurrence sequence-to-sequence model for speech recognition (2018)

* ## More recent papers in NLP:
* Generating Long Sequences with Sparse Transformers (2019)
* Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context (2019)
* XLNet: Generalized Autoregressive Pretraining for Language Understanding (2019)
* Language Models are Unsupervised Multitask Learners (GPT-2, 2019)
* Language Models are Few-Shot Learners (GPT-3, 2020)
* ALBERT: A Lite BERT for Self-Supervised Learning of Language Representations (2020)
* To keep up with the newest papers and their code bases go to paperswithcode.com. For example, check out the raked list of state of the art models for Language Modelling on Penn Treebank.

## Read or watch:

![img1](https://s3.eu-west-3.amazonaws.com/hbtn.intranet/uploads/medias/2020/11/52f1c0b37c7c58d30706b7e24bb7534b5a5889db.jpg?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIA4MYA5JM5DUTZGMZG%2F20230213%2Feu-west-3%2Fs3%2Faws4_request&X-Amz-Date=20230213T144917Z&X-Amz-Expires=86400&X-Amz-SignedHeaders=host&X-Amz-Signature=ddb0891e92db77878e82ab7ea744d76fadb9312d86cab363bcb645e5196eb66c)

## Extra resources around relational:

* ## Zendesk Articles
* For this project, we will be using a collection of Holberton USA Zendesk Articles, [ZendeskArticles.zip.](https://s3.eu-west-3.amazonaws.com/hbtn.intranet/uploads/misc/2020/11/c15a067b44a328c7d5a03c79070b7865f444d1e3.zip?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIA4MYA5JM5DUTZGMZG%2F20230213%2Feu-west-3%2Fs3%2Faws4_request&X-Amz-Date=20230213T144917Z&X-Amz-Expires=345600&X-Amz-SignedHeaders=host&X-Amz-Signature=3e08ae68133cbd76d092962bd9b6a02ca1be8fb562c38e0f322331416cb38501)

## More Info

## Files included

| File                 | Details                                    |
|--------------------- | ------------------------------------------ |
| [0-qa.py]() |	       |
| [1-loop.py]() |	       |
| [2-qa.py]() |	       |
| [3-semantic_search.py]() |	       |
| [4-qa.py]() |	       |
| []() |	       |
| []() |	       |
| []() |	       |
| []() |	       |
| []() |	       |


## Author
***
*Holberton School Student*

Juan Sebastian Posada  - [Github](https://github.com/Juansepo13) - [Twiter](https://twitter.com/@JuanSeb35904130)