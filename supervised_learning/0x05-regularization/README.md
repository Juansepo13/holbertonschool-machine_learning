# 0x05. Regularization
***
## This is a README.md for the repository
### In GitHub [0x05-regularization]()
```
For Holberton School
Cohort 16.
```
## General

![img](https://holbertonintranet.s3.amazonaws.com/uploads/medias/2019/6/689c11afbc30eaa89b50.jpg?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIARDDGGGOU5BHMTQX4%2F20220906%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220906T151808Z&X-Amz-Expires=86400&X-Amz-SignedHeaders=host&X-Amz-Signature=f741c8c5f5774bb120fe8a1b60df0b9b9c5c685992262fba0097c4146df9779b)

## Resources
* Regularization (mathematics)
* An Overview of Regularization Techniques in Deep Learning (up to A case study on MNIST data with keras excluded)
* L2 Regularization and Back-Propagation
* Intuitions on L1 and L2 Regularisation
* Analysis of Dropout
* Early stopping
* How to use early stopping properly for training deep neural network?
* Data Augmentation | How to use Deep Learning when you have Limited Dataâ€Š
* deeplearning.ai videos (Note: I suggest watching these video at 1.5x - 2x speed):
* Regularization
* Why Regularization Reduces Overfitting
* Dropout Regularization
* Understanding Dropout
* Other Regularization Methods

* numpy.linalg.norm
* numpy.random.binomial
* tf.keras.regularizers.L2
* tf.layers.Dense
* tf.losses.get_regularization_loss
* tf.layers.Dropout
* Dropout: A Simple Way to Prevent Neural Networks from Overfitting
* Early Stopping - but when?
* L2 Regularization versus Batch and * * Weight Normalization
## Read or watch:

![Alt text]()

## Extra resources around relational:

## More Info

## Files included

| File                 | Details                                    |
|--------------------- | ------------------------------------------ |
| []() |	       |
| []() |	       |
| []() |	       |
| []() |	       |
| []() |	       |
| []() |	       |
| []() |	       |
| []() |	       |
| []() |	       |
| []() |	       |


## Author
***
*Holberton School Student*

Juan Sebastian Posada  - [Github](https://github.com/Juansepo13) - [Twiter](https://twitter.com/@JuanSeb35904130)